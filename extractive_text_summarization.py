# -*- coding: utf-8 -*-
"""Extractive-Text-Summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QILbC83fw6TIc5Mtacfa3ch75Qa6BEXy

**Extractive Summarization** is a text summarization technique that identifies the most important sentences or phrases in the source text and directly extracts them to form a concise summary. Unlike abstractive summarization, which generates new sentences to summarize the content, extractive summarization retains the original sentences, ensuring that the output is grammatically correct and contextually accurate.
"""

# Import necessary libraries
import os
import numpy as np
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

# Ensure NLTK resources are downloaded (you only need to run this once)
nltk.download('punkt')



#Define the extractive summarization function
def extractive_summary(text, num_sentences=3):
      """
    Generate an extractive summary of the input text.

    Parameters:
        text (str): The original text to be summarized.
        num_sentences (int): The number of sentences to include in the summary.

    Returns:
        str: The summarized text containing the most important sentences.
    """
    ## Step 1: Tokenize the input text into sentences
    sentences = nltk.sent_tokenize(text)

    ## Step 2: Compute the TF-IDF matrix for the sentences
    # This converts sentences into numerical vectors based on word importanc
    vectorizer = Tfidfvectorizer()
    tfidf_matrix = vectorizer.fit_transform(sentences)

    ## Step 3: Calculate cosine similarity between sentences
    # This measures how similar each sentence is to every other sentence
    similarity_matrix = cosime_similarity(tfidf_matrix, tfidf_matrix)

    ## Step 4: Rank sentences based on their similarity scores
    # Sum the similarity scores for each sentence to determine its importance
    sentence_scores = similarity_matrix.sum(axis=1)

    # Step 5: Select the top N sentences with the highest scores
    # These sentences are considered the most important for the summary
    ranked_sentences = [sentences[i] for i in np.argsort(-sentence_scores)[:num_sentences]]

    # Step 6: Combine the selected sentences into a single string as the summary
    summary = ' '.join(ranked_sentences)

    return summary

import os

def load_datasets(articles_folder, summaries_folder):
    """
    Load articles and summaries from the specified folders.

    Args:
    - articles_folder (str): Path to the folder containing news articles.
    - summaries_folder (str): Path to the folder containing summaries.

    Returns:
    - articles_dict (dict): Dictionary with file names as keys and article texts as values.
    - summaries_dict (dict): Dictionary with file names as keys and summary texts as values.
    """
    # Initialize dictionaries to store articles and summaries
    articles_dict = {}
    summaries_dict = {}

    # Load articles
    for filename in os.listdir(articles_folder):
        file_path = os.path.join(articles_folder, filename)
        if os.path.isfile(file_path):
            with open(file_path, 'r', encoding='utf-8') as file:
                articles_dict[filename] = file.read()

    # Load summaries
    for filename in os.listdir(summaries_folder):
        file_path = os.path.join(summaries_folder, filename)
        if os.path.isfile(file_path):
            with open(file_path, 'r', encoding='utf-8') as file:
                summaries_dict[filename] = file.read()

    return articles_dict, summaries_dict

# Example usage
articles_folder = '/content/drive/My Drive/Projects/NewsSummarization/BBC News Summary/News Articles'
summaries_folder = '/content/drive/My Drive/Projects/NewsSummarization/BBC News Summary/Summaries'

articles_dict, summaries_dict = load_datasets(articles_folder, summaries_folder)

# Print the number of articles and summaries loaded
print(f"Loaded {len(articles_dict)} articles and {len(summaries_dict)} summaries.")

# Print the first 500 characters of the original article for context
print("Original Article:", articles[first_article_filename][:500])

# Generate a summary using the extractive summarization function
generated_summary = extractive_summary(articles[first_article_filename])

# Print the generated summary
print("\nGenerated Summary:", generated_summary)

# Print the reference summary from the dataset for comparison
print("\nReference Summary:", summaries[first_article_filename])